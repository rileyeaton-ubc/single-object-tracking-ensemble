
@inproceedings{noauthor_visual_2010,
	title = {Visual object tracking using adaptive correlation filters},
	url = {https://scispace.com/papers/visual-object-tracking-using-adaptive-correlation-filters-1xuhtpe358},
	doi = {10.1109/CVPR.2010.5539960},
	abstract = {Although not commonly used, correlation filters can track complex objects through rotations, occlusions and other distractions at over 20 times the rate of current state-of-the-art techniques. The oldest and simplest correlation filters use simple templates and generally fail when applied to tracking. More modern approaches such as ASEF and UMACE perform better, but their training needs are poorly suited to tracking. Visual tracking requires robust filters to be trained from a single frame and dynamically adapted as the appearance of the target object changes. This paper presents a new type of correlation filter, a Minimum Output Sum of Squared Error (MOSSE) filter, which produces stable correlation filters when initialized using a single frame. A tracker based upon MOSSE filters is robust to variations in lighting, scale, pose, and nonrigid deformations while operating at 669 frames per second. Occlusion is detected based upon the peak-to-sidelobe ratio, which enables the tracker to pause and resume where it left off when the object reappears.},
	language = {en},
	urldate = {2025-02-25},
	booktitle = {{SciSpace} - {Paper}},
	publisher = {IEEE},
	month = jun,
	year = {2010},
	pages = {2544--2550},
	file = {Full Text PDF:C\:\\Users\\Riley\\Zotero\\storage\\CG2S82M3\\2010 - Visual object tracking using adaptive correlation filters.pdf:application/pdf},
}

@article{zhu_efficient_2021,
	title = {Efficient and {Practical} {Correlation} {Filter} {Tracking}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/21/3/790},
	doi = {10.3390/s21030790},
	abstract = {Visual tracking is a basic task in many applications. However, the heavy computation and low speed of many recent trackers limit their applications in some computing power restricted scenarios. On the other hand, the simple update scheme of most correlation filter-based trackers restricts their robustness during target deformation and occlusion. In this paper, we explore the update scheme of correlation filter-based trackers and propose an efficient and adaptive training sample update scheme. The training sample extracted in each frame is updated to the training set according to its distance between existing samples measured with a difference hashing algorithm or discarded according to tracking result reliability. In addition, we expand our new tracker to long-term tracking. On the basis of the proposed model updating mechanism, we propose a new tracking state discrimination mechanism to accurately judge tracking failure, and resume tracking after the target is recovered. Experiments on OTB-2015, Temple Color 128 and UAV123 (including UAV20L) demonstrate that our tracker performs favorably against state-of-the-art trackers with light computation and runs over 100 fps on desktop computer with Intel i7-8700 CPU(3.2 GHz).},
	language = {en},
	number = {3},
	urldate = {2025-02-25},
	journal = {Sensors},
	author = {Zhu, Chengfei and Jiang, Shan and Li, Shuxiao and Lan, Xiaosong},
	month = jan,
	year = {2021},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {correlation filter, long-term tracking, model update, visual tracking},
	pages = {790},
	file = {Full Text PDF:C\:\\Users\\Riley\\Zotero\\storage\\EBQ9PY2J\\Zhu et al. - 2021 - Efficient and Practical Correlation Filter Tracking.pdf:application/pdf},
}

@inproceedings{danelljan_learning_2015,
	title = {Learning {Spatially} {Regularized} {Correlation} {Filters} for {Visual} {Tracking}},
	url = {http://arxiv.org/abs/1608.05571},
	doi = {10.1109/ICCV.2015.490},
	abstract = {Robust and accurate visual tracking is one of the most challenging computer vision problems. Due to the inherent lack of training data, a robust approach for constructing a target appearance model is crucial. Recently, discriminatively learned correlation filters (DCF) have been successfully applied to address this problem for tracking. These methods utilize a periodic assumption of the training samples to efficiently learn a classifier on all patches in the target neighborhood. However, the periodic assumption also introduces unwanted boundary effects, which severely degrade the quality of the tracking model. We propose Spatially Regularized Discriminative Correlation Filters (SRDCF) for tracking. A spatial regularization component is introduced in the learning to penalize correlation filter coefficients depending on their spatial location. Our SRDCF formulation allows the correlation filters to be learned on a significantly larger set of negative training samples, without corrupting the positive samples. We further propose an optimization strategy, based on the iterative Gauss-Seidel method, for efficient online learning of our SRDCF. Experiments are performed on four benchmark datasets: OTB-2013, ALOV++, OTB-2015, and VOT2014. Our approach achieves state-of-the-art results on all four datasets. On OTB-2013 and OTB-2015, we obtain an absolute gain of 8.0\% and 8.2\% respectively, in mean overlap precision, compared to the best existing trackers.},
	urldate = {2025-02-25},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Danelljan, Martin and Häger, Gustav and Khan, Fahad Shahbaz and Felsberg, Michael},
	month = dec,
	year = {2015},
	note = {arXiv:1608.05571 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {4310--4318},
	file = {Preprint PDF:C\:\\Users\\Riley\\Zotero\\storage\\5MQZNU2L\\Danelljan et al. - 2015 - Learning Spatially Regularized Correlation Filters for Visual Tracking.pdf:application/pdf;Snapshot:C\:\\Users\\Riley\\Zotero\\storage\\6X9SLWEG\\1608.html:text/html},
}

@misc{li_learning_2018,
	title = {Learning {Spatial}-{Temporal} {Regularized} {Correlation} {Filters} for {Visual} {Tracking}},
	url = {http://arxiv.org/abs/1803.08679},
	doi = {10.48550/arXiv.1803.08679},
	abstract = {Discriminative Correlation Filters (DCF) are efficient in visual tracking but suffer from unwanted boundary effects. Spatially Regularized DCF (SRDCF) has been suggested to resolve this issue by enforcing spatial penalty on DCF coefficients, which, inevitably, improves the tracking performance at the price of increasing complexity. To tackle online updating, SRDCF formulates its model on multiple training images, further adding difficulties in improving efficiency. In this work, by introducing temporal regularization to SRDCF with single sample, we present our spatial-temporal regularized correlation filters (STRCF). Motivated by online Passive-Agressive (PA) algorithm, we introduce the temporal regularization to SRDCF with single sample, thus resulting in our spatial-temporal regularized correlation filters (STRCF). The STRCF formulation can not only serve as a reasonable approximation to SRDCF with multiple training samples, but also provide a more robust appearance model than SRDCF in the case of large appearance variations. Besides, it can be efficiently solved via the alternating direction method of multipliers (ADMM). By incorporating both temporal and spatial regularization, our STRCF can handle boundary effects without much loss in efficiency and achieve superior performance over SRDCF in terms of accuracy and speed. Experiments are conducted on three benchmark datasets: OTB-2015, Temple-Color, and VOT-2016. Compared with SRDCF, STRCF with hand-crafted features provides a 5 times speedup and achieves a gain of 5.4\% and 3.6\% AUC score on OTB-2015 and Temple-Color, respectively. Moreover, STRCF combined with CNN features also performs favorably against state-of-the-art CNN-based trackers and achieves an AUC score of 68.3\% on OTB-2015.},
	urldate = {2025-02-25},
	publisher = {arXiv},
	author = {Li, Feng and Tian, Cheng and Zuo, Wangmeng and Zhang, Lei and Yang, Ming-Hsuan},
	month = mar,
	year = {2018},
	note = {arXiv:1803.08679 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\Riley\\Zotero\\storage\\YGBTCCIA\\Li et al. - 2018 - Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking.pdf:application/pdf;Snapshot:C\:\\Users\\Riley\\Zotero\\storage\\IY5HYKTZ\\1803.html:text/html},
}

@article{danelljan_discriminative_2017,
	title = {Discriminative {Scale} {Space} {Tracking}},
	volume = {39},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/document/7569092},
	doi = {10.1109/TPAMI.2016.2609928},
	abstract = {Accurate scale estimation of a target is a challenging research problem in visual object tracking. Most state-of-the-art methods employ an exhaustive scale search to estimate the target size. The exhaustive search strategy is computationally expensive and struggles when encountered with large scale variations. This paper investigates the problem of accurate and robust scale estimation in a tracking-by-detection framework. We propose a novel scale adaptive tracking approach by learning separate discriminative correlation filters for translation and scale estimation. The explicit scale filter is learned online using the target appearance sampled at a set of different scales. Contrary to standard approaches, our method directly learns the appearance change induced by variations in the target scale. Additionally, we investigate strategies to reduce the computational cost of our approach. Extensive experiments are performed on the OTB and the VOT2014 datasets. Compared to the standard exhaustive scale search, our approach achieves a gain of 2.5 percent in average overlap precision on the OTB dataset. Additionally, our method is computationally efficient, operating at a 50 percent higher frame rate compared to the exhaustive scale search. Our method obtains the top rank in performance by outperforming 19 state-of-the-art trackers on OTB and 37 state-of-the-art trackers on VOT2014.},
	number = {8},
	urldate = {2025-02-25},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Danelljan, Martin and Häger, Gustav and Khan, Fahad Shahbaz and Felsberg, Michael},
	month = aug,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Correlation, correlation filters, Decision support systems, Estimation, Robustness, scale estimation, Standards, Target tracking, Visual tracking, Visualization},
	pages = {1561--1575},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Riley\\Zotero\\storage\\GWVSWNZF\\7569092.html:text/html;Submitted Version:C\:\\Users\\Riley\\Zotero\\storage\\98J6R5M8\\Danelljan et al. - 2017 - Discriminative Scale Space Tracking.pdf:application/pdf},
}

@incollection{chumachenko_chapter_2022,
	title = {Chapter 11 - {Object} detection and tracking},
	isbn = {978-0-323-85787-1},
	url = {https://www.sciencedirect.com/science/article/pii/B9780323857871000166},
	abstract = {The availability of an increasing amount of computational power and large-scale public data sets has driven the field of object detection and tracking with an unprecedented development speed, finding applications in many areas. This chapter surveys the most prominent methods in the field. We first formulate the problems of object detection and single and multiple object tracking, and then present the most relevant methodologies which have successfully been developed to solve these problems. The chapter includes a comprehensive treatment of two-stage, one-stage and anchor-free object detection methods. Both single object and multiple object tracking methods are reviewed in the chapter. The former includes methods based on correlation filters and deep learning, including similarity learning, whereas the latter present online and offline multiple object tracking techniques. Online methods, including deep features driven and detection-based methods, rely on visual representations, while offline methods are mostly based on graph optimization.},
	urldate = {2025-02-25},
	booktitle = {Deep {Learning} for {Robot} {Perception} and {Cognition}},
	publisher = {Academic Press},
	author = {Chumachenko, Kateryna and Gabbouj, Moncef and Iosifidis, Alexandros},
	editor = {Iosifidis, Alexandros and Tefas, Anastasios},
	month = jan,
	year = {2022},
	doi = {10.1016/B978-0-32-385787-1.00016-6},
	keywords = {Multiple object tracking, Object detection, Object tracking, Single object tracking, Visual object tracking},
	pages = {243--278},
	file = {ScienceDirect Snapshot:C\:\\Users\\Riley\\Zotero\\storage\\XUUBUANC\\B9780323857871000166.html:text/html},
}

@incollection{danelljan_beyond_2016,
	title = {Beyond {Correlation} {Filters}: {Learning} {Continuous} {Convolution} {Operators} for {Visual} {Tracking}},
	volume = {9909},
	shorttitle = {Beyond {Correlation} {Filters}},
	url = {http://arxiv.org/abs/1608.03773},
	abstract = {Discriminative Correlation Filters (DCF) have demonstrated excellent performance for visual object tracking. The key to their success is the ability to efficiently exploit available negative data by including all shifted versions of a training sample. However, the underlying DCF formulation is restricted to single-resolution feature maps, significantly limiting its potential. In this paper, we go beyond the conventional DCF framework and introduce a novel formulation for training continuous convolution filters. We employ an implicit interpolation model to pose the learning problem in the continuous spatial domain. Our proposed formulation enables efficient integration of multi-resolution deep feature maps, leading to superior results on three object tracking benchmarks: OTB-2015 (+5.1\% in mean OP), Temple-Color (+4.6\% in mean OP), and VOT2015 (20\% relative reduction in failure rate). Additionally, our approach is capable of sub-pixel localization, crucial for the task of accurate feature point tracking. We also demonstrate the effectiveness of our learning formulation in extensive feature point tracking experiments. Code and supplementary material are available at http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/index.html.},
	urldate = {2025-02-25},
	author = {Danelljan, Martin and Robinson, Andreas and Khan, Fahad Shahbaz and Felsberg, Michael},
	year = {2016},
	doi = {10.1007/978-3-319-46454-1_29},
	note = {arXiv:1608.03773 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {472--488},
	file = {Preprint PDF:C\:\\Users\\Riley\\Zotero\\storage\\SK7JA5W4\\Danelljan et al. - 2016 - Beyond Correlation Filters Learning Continuous Convolution Operators for Visual Tracking.pdf:application/pdf;Snapshot:C\:\\Users\\Riley\\Zotero\\storage\\HR7G6PII\\1608.html:text/html},
}

@inproceedings{lowe_object_1999,
	title = {Object recognition from local scale-invariant features},
	volume = {2},
	url = {https://ieeexplore.ieee.org/document/790410},
	doi = {10.1109/ICCV.1999.790410},
	abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds.},
	urldate = {2025-02-25},
	booktitle = {Proceedings of the {Seventh} {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Lowe, D.G.},
	month = sep,
	year = {1999},
	keywords = {Computer science, Electrical capacitance tomography, Filters, Image recognition, Layout, Lighting, Neurons, Object recognition, Programmable logic arrays, Reactive power},
	pages = {1150--1157 vol.2},
}

@inproceedings{cobos_fast_2019,
	title = {A fast multi-object tracking system using an object detector ensemble},
	url = {https://ieeexplore.ieee.org/document/8781972},
	doi = {10.1109/ColCACI.2019.8781972},
	abstract = {Multiple-Object Tracking (MOT) is of crucial importance for applications such as retail video analytics and video surveillance. Object detectors are often the computational bottleneck of modern MOT systems, limiting their use for real-time applications. In this paper, we address this issue by leveraging on an ensemble of detectors, each running every f frames. We measured the performance of our system in the MOT16 benchmark. The proposed model surpassed other online entries of the MOT16 challenge in speed, while maintaining an acceptable accuracy.},
	urldate = {2025-02-25},
	booktitle = {2019 {IEEE} {Colombian} {Conference} on {Applications} in {Computational} {Intelligence} ({ColCACI})},
	author = {Cobos, Richard and Hernandez, Jefferson and Abad, Andres G.},
	month = jun,
	year = {2019},
	keywords = {Computational modeling, Detectors, ensemble, Kalman filters, Mathematical model, multi-object tracking, object detection, Prediction algorithms, Real-time systems, Tracking},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Riley\\Zotero\\storage\\6E8EB2P3\\8781972.html:text/html;Submitted Version:C\:\\Users\\Riley\\Zotero\\storage\\UMB7JU4L\\Cobos et al. - 2019 - A fast multi-object tracking system using an object detector ensemble.pdf:application/pdf},
}

@misc{du_ensemblemot_2023,
	title = {{EnsembleMOT}: {A} {Step} towards {Ensemble} {Learning} of {Multiple} {Object} {Tracking}},
	shorttitle = {{EnsembleMOT}},
	url = {http://arxiv.org/abs/2210.05278},
	doi = {10.48550/arXiv.2210.05278},
	abstract = {Multiple Object Tracking (MOT) has rapidly progressed in recent years. Existing works tend to design a single tracking algorithm to perform both detection and association. Though ensemble learning has been exploited in many tasks, i.e, classification and object detection, it hasn't been studied in the MOT task, which is mainly caused by its complexity and evaluation metrics. In this paper, we propose a simple but effective ensemble method for MOT, called EnsembleMOT, which merges multiple tracking results from various trackers with spatio-temporal constraints. Meanwhile, several post-processing procedures are applied to filter out abnormal results. Our method is model-independent and doesn't need the learning procedure. What's more, it can easily work in conjunction with other algorithms, e.g., tracklets interpolation. Experiments on the MOT17 dataset demonstrate the effectiveness of the proposed method. Codes are available at https://github.com/dyhBUPT/EnsembleMOT.},
	urldate = {2025-02-25},
	publisher = {arXiv},
	author = {Du, Yunhao and Liu, Zihang and Su, Fei},
	month = feb,
	year = {2023},
	note = {arXiv:2210.05278 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\Riley\\Zotero\\storage\\36WVWI8Y\\Du et al. - 2023 - EnsembleMOT A Step towards Ensemble Learning of Multiple Object Tracking.pdf:application/pdf;Snapshot:C\:\\Users\\Riley\\Zotero\\storage\\J5FLWZ3V\\2210.html:text/html},
}

@misc{otb_2015_papers_nodate,
	title = {Papers with {Code} - {OTB}-2015 {Dataset}},
	url = {https://paperswithcode.com/dataset/otb-2015},
	abstract = {OTB-2015, also referred as Visual Tracker Benchmark, is a visual tracking dataset. It contains 100 commonly used video sequences for evaluating visual tracking.},
	language = {en},
	urldate = {2025-02-25},
	file = {Snapshot:C\:\\Users\\Riley\\Zotero\\storage\\Y9EJFZMG\\otb-2015.html:text/html},
}

@misc{lasot_papers_nodate,
	title = {Papers with {Code} - {LaSOT} {Dataset}},
	url = {https://paperswithcode.com/dataset/lasot},
	abstract = {LaSOT is a high-quality benchmark for Large-scale Single Object Tracking. LaSOT consists of 1,400 sequences with more than 3.5M frames in total. Each frame in these sequences is carefully and manually annotated with a bounding box, making LaSOT one of the largest densely annotated
tracking benchmark. The average video length of LaSOT
is more than 2,500 frames, and each sequence comprises
various challenges deriving from the wild where target objects may disappear and re-appear again in the view.},
	language = {en},
	urldate = {2025-02-25},
	file = {Snapshot:C\:\\Users\\Riley\\Zotero\\storage\\XIVKM5D9\\lasot.html:text/html},
}
